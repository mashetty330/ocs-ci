import pytest
import logging

# import time

# from ocs_ci.ocs.resources.storage_cluster import add_capacity_lso
# from ocs_ci.ocs.flowtest import FlowOperations
# from ocs_ci.ocs.node import get_nodes_having_label, drain_nodes, schedule_nodes
from ocs_ci.ocs.resources.ocs import OCS

from ocs_ci.ocs import constants
from ocs_ci.helpers.sanity_helpers import Sanity
from ocs_ci.ocs.node import get_nodes_having_label
from ocs_ci.ocs.resources.pod import wait_for_storage_pods

logger = logging.getLogger(__name__)


class TestBaseOperations:
    """
    This test test will perform the base operations while running
    both logwriter and logreader workloads in the background

    """

    @pytest.fixture(autouse=True)
    def init_sanity(self):
        """
        Initialize Sanity instance

        """
        self.sanity_helpers = Sanity()

    def test_base_ops_with_workloads(
        self,
        nodes,
        pvc_factory,
        pod_factory,
        bucket_factory,
        rgw_bucket_factory,
        add_nodes,
    ):
        """
        This test will perform base operations like add capacity,
        node drain etc. And then make sure there is no data loss,
        data corruption is seen

        """
        data_zones = ["data-1", "data-2"]
        # flow_ops = FlowOperations()
        # logwriter_dep, logreader_job = setup_logwriter_cephfs_workload_factory
        #
        # # Note all the log files generated by the instances
        #
        # # Generate 5 mins worth of logs
        #
        # # Add capacity
        # # osd_pods_before, restart_count_before = flow_ops.add_capacity_entry_criteria()
        # # add_capacity_lso()
        # # logger.info("Added capacity successfully")
        # # flow_ops.add_capacity_exit_criteria(restart_count_before, osd_pods_before)
        #
        # # Node drain
        label = f"node-role.kubernetes.io/worker=,topology.kubernetes.io/zone={data_zones[0]}"
        data_nodes = [
            OCS(**node)
            for node in get_nodes_having_label(
                label=label,
            )
        ]
        # flow_ops.validate_cluster(
        #     node_status=True, pod_status=True, operation_name="Node drain"
        # )
        # drain_nodes(data_nodes[0].name)
        # schedule_nodes(data_nodes[0].name)
        #
        # # Prolonged node drain
        # flow_ops.validate_cluster(
        #     node_status=True, pod_status=True, operation_name="Node drain"
        # )
        # drain_nodes(data_nodes[1].name)
        # time.sleep(600)
        # schedule_nodes(data_nodes[1].name)

        # device replacement
        # from ocs_ci.ocs import osd_operations
        #
        # osd_operations.osd_device_replacement(nodes)
        #
        # self.sanity_helpers.create_resources(
        #     pvc_factory, pod_factory, bucket_factory, rgw_bucket_factory
        # )

        # add node to the data-zone and turn off the original node
        orginal_node = data_nodes[0]
        logger.info(orginal_node)
        add_nodes(
            node_count=1,
            ocs_nodes=True,
            other_labels=[f"{constants.ZONE_LABEL}={data_zones[0]}"],
        )

        nodes.stop_nodes(nodes=[data_nodes[0]])
        wait_for_storage_pods(timeout=600)
